{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc98e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "%run Functions.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa107d9",
   "metadata": {},
   "source": [
    "First the data is imported from the Interval infor csv file saved on the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1513f457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abnormal</th>\n",
       "      <th>Result</th>\n",
       "      <th>R-R Interval</th>\n",
       "      <th>R Height</th>\n",
       "      <th>R_Onset-Rpeak</th>\n",
       "      <th>Q-Q Interval</th>\n",
       "      <th>Q Height</th>\n",
       "      <th>P_Onset-P_Offset</th>\n",
       "      <th>P Height</th>\n",
       "      <th>P-P Interval</th>\n",
       "      <th>P_Onset-Ppeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>285.0</td>\n",
       "      <td>1188.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abnormal Result  R-R Interval  R Height  R_Onset-Rpeak  Q-Q Interval  \\\n",
       "0       0.0      N           0.0    1212.0           56.0           0.0   \n",
       "1       0.0      N         292.0    1201.0           50.0         294.0   \n",
       "2       0.0      N         284.0    1186.0           54.0         285.0   \n",
       "3       0.0      N         285.0    1188.0           59.0         284.0   \n",
       "4       0.0      N         284.0    1201.0           56.0         283.0   \n",
       "\n",
       "   Q Height  P_Onset-P_Offset  P Height  P-P Interval  P_Onset-Ppeak  \n",
       "0     953.0              61.0     978.0           0.0           28.0  \n",
       "1     947.0              60.0     976.0         295.0           28.0  \n",
       "2     946.0              65.0     971.0         280.0           28.0  \n",
       "3     948.0              66.0     973.0         283.0           29.0  \n",
       "4     952.0              64.0     977.0         285.0           28.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA=pd.read_csv(\"IntervalInfo.csv\")\n",
    "DATA=DATA[DATA.columns[2:]]\n",
    "\n",
    "DATA.dropna(inplace=True)\n",
    "DATA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048c972",
   "metadata": {},
   "source": [
    "Then four decision tree classifiers are called which will all be trained on different subsets of the data to determine the optimal information to put into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced39dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = tree.DecisionTreeClassifier()\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "model4 = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7352a0",
   "metadata": {},
   "source": [
    "the four subsets of data are extracted from the features data frame. the subsets include all of the data, data with an equal ratio of abnormal to normal information, a dataset consisting of only abnormal values, and a dataset where some of the features are omitted due to uncertainty in the reliability of the information involved due to issues in the preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fce293db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17067\n",
      "34134\n",
      "       Abnormal\n",
      "0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "...         ...\n",
      "71472       0.0\n",
      "71473       0.0\n",
      "71474       0.0\n",
      "71475       0.0\n",
      "71476       0.0\n",
      "\n",
      "[71477 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "inputs = DATA.drop([\"Result\",\"Abnormal\"],axis='columns')\n",
    "Target = DATA[[\"Abnormal\"]]\n",
    "\n",
    "NumAbnormal=0\n",
    "for i in DATA[\"Abnormal\"]:\n",
    "    if (i==1):\n",
    "        NumAbnormal+=1\n",
    "print(NumAbnormal) #23% abnormal\n",
    "\n",
    "rows_to_remove=DATA.index[DATA['Abnormal'] == 0].tolist()\n",
    "inputs2 = inputs.drop(rows_to_remove[NumAbnormal:],axis='rows')\n",
    "Target2 = Target.drop(rows_to_remove[NumAbnormal:],axis='rows')\n",
    "\n",
    "inputs3 = inputs.drop(rows_to_remove,axis='rows')\n",
    "Target3 = Target.drop(rows_to_remove,axis='rows')\n",
    "\n",
    "inputs4 = DATA.drop([\"Result\",\"Abnormal\",\"R Height\",\"Q Height\", \"P Height\"],axis='columns')\n",
    "Target4 = DATA[[\"Abnormal\"]]\n",
    "\n",
    "print(len(inputs2))\n",
    "\n",
    "print(Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85561c1",
   "metadata": {},
   "source": [
    "Training and testing data is then created for each of the datasets so that the four models can be trained accordingly. the set is randomized in the train_test_split function and this is important due to the distribution in the actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "904dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(inputs, Target,test_size=0.3)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(inputs2, Target2,test_size=0.3)\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(inputs3, Target3,test_size=0.3)\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(inputs4, Target4,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c643074",
   "metadata": {},
   "source": [
    "Here we train and test the first model. this model is trained on the entire dataset then tested on all of them,\n",
    "doing this shows a greater picture for the actual accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc58c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9312628240999814 Regular\n",
      "0.9713895127428962 50/50\n",
      "0.9586018355789885 Abnormal\n",
      "0.9072564883967363 AUC1\n",
      "0.9713630126766972 AUC2\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X=x_train1,y=y_train1)\n",
    "print(model1.score(x_test1,y_test1),\"Regular\")\n",
    "print(model1.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model1.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model1.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model1.predict(x_test2)),\"AUC2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41c0d7",
   "metadata": {},
   "source": [
    "The second model is trained with a dataset consisting of an even distribution of normal and abnormal heart beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1209ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6616769259466517 Regular\n",
      "0.9474660677668196 50/50\n",
      "0.9869166178480765 Abnormal\n",
      "0.7720916603271063 AUC1\n",
      "0.9474678681259804 AUC2\n"
     ]
    }
   ],
   "source": [
    "#for inputs with even normal and abnormal\n",
    "\n",
    "model2.fit(X=x_train2,y=y_train2)\n",
    "print(model2.score(x_test1,y_test1),\"Regular\")\n",
    "print(model2.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model2.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model2.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model2.predict(x_test2)),\"AUC2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e2aa0",
   "metadata": {},
   "source": [
    "The third model is trained on a dataset consisting of only abnormal data, this was done mostly as a test to see what would happen by experimenting with the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5c3234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2389945905614624 Regular\n",
      "0.49917000292940145 50/50\n",
      "1.0 Abnormal\n",
      "0.5 AUC1\n",
      "0.5 AUC2\n"
     ]
    }
   ],
   "source": [
    "model3.fit(X=x_train3,y=y_train3)\n",
    "print(model3.score(x_test1,y_test1),\"Regular\")\n",
    "print(model3.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model3.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model3.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model3.predict(x_test2)),\"AUC2\")\n",
    "#print(roc_auc_score(y_test3, model1.predict(x_test3)),\"AUC3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdd6c6",
   "metadata": {},
   "source": [
    "The final model is trained on the entire dataset with the omission of peak height features. this is due to having less confidence in these values. accuracy regarding these features would likely improve with normalizing on the waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "992284a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8816452154448797 Regular\n",
      "0.8392776790770925 AUC1\n"
     ]
    }
   ],
   "source": [
    "model4.fit(X=x_train4,y=y_train4)\n",
    "print(model4.score(x_test4,y_test4),\"Regular\")\n",
    "print(roc_auc_score(y_test4, model4.predict(x_test4)),\"AUC1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ef6ac",
   "metadata": {},
   "source": [
    "It can be seen that model1, taking all of the provided data has the highest accuracy when tested against all of the subsets of data. This was the expected conclusion but it is still useful to show proof of this fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d1c6e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5ec59428158f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#_ = tree.plot_tree(model,feature_names=inputs.columns,class_names=CLASSNAMES)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x7200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CLASSNAMES=[\"0\",\"1\"]\n",
    "fig = plt.figure(figsize=(100,100))\n",
    "#_ = tree.plot_tree(model,feature_names=inputs.columns,class_names=CLASSNAMES)\n",
    "_ = tree.plot_tree(model,feature_names=inputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee2d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
