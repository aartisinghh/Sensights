{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc98e4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "%run Functions.ipynb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa107d9",
   "metadata": {},
   "source": [
    "First the data is imported from the Interval infor csv file saved on the branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1513f457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71477\n",
      "71477\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abnormal</th>\n",
       "      <th>Result</th>\n",
       "      <th>R-R Interval</th>\n",
       "      <th>R Height</th>\n",
       "      <th>R_Onset-Rpeak</th>\n",
       "      <th>Q-Q Interval</th>\n",
       "      <th>Q Height</th>\n",
       "      <th>P_Onset-P_Offset</th>\n",
       "      <th>P Height</th>\n",
       "      <th>P-P Interval</th>\n",
       "      <th>P_Onset-Ppeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>953.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>947.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>976.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>285.0</td>\n",
       "      <td>1188.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>948.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>952.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>977.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Abnormal Result  R-R Interval  R Height  R_Onset-Rpeak  Q-Q Interval  \\\n",
       "0       0.0      N           0.0    1212.0           56.0           0.0   \n",
       "1       0.0      N         292.0    1201.0           50.0         294.0   \n",
       "2       0.0      N         284.0    1186.0           54.0         285.0   \n",
       "3       0.0      N         285.0    1188.0           59.0         284.0   \n",
       "4       0.0      N         284.0    1201.0           56.0         283.0   \n",
       "\n",
       "   Q Height  P_Onset-P_Offset  P Height  P-P Interval  P_Onset-Ppeak  \n",
       "0     953.0              61.0     978.0           0.0           28.0  \n",
       "1     947.0              60.0     976.0         295.0           28.0  \n",
       "2     946.0              65.0     971.0         280.0           28.0  \n",
       "3     948.0              66.0     973.0         283.0           29.0  \n",
       "4     952.0              64.0     977.0         285.0           28.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA=pd.read_csv(\"IntervalInfo.csv\")\n",
    "DATA=DATA[DATA.columns[2:]]\n",
    "\n",
    "DATA.dropna(inplace=True)\n",
    "DATA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048c972",
   "metadata": {},
   "source": [
    "Then four decision tree classifiers are called which will all be trained on different subsets of the data to determine the optimal information to put into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ced39dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = tree.DecisionTreeClassifier()\n",
    "model2 = tree.DecisionTreeClassifier()\n",
    "model3 = tree.DecisionTreeClassifier()\n",
    "model4 = tree.DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7352a0",
   "metadata": {},
   "source": [
    "the four subsets of data are extracted from the features data frame. the subsets include all of the data, data with an equal ratio of abnormal to normal information, a dataset consisting of only abnormal values, and a dataset where some of the features are omitted due to uncertainty in the reliability of the information involved due to issues in the preprocessing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fce293db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17067\n",
      "34134\n",
      "       Abnormal\n",
      "0           0.0\n",
      "1           0.0\n",
      "2           0.0\n",
      "3           0.0\n",
      "4           0.0\n",
      "...         ...\n",
      "71472       0.0\n",
      "71473       0.0\n",
      "71474       0.0\n",
      "71475       0.0\n",
      "71476       0.0\n",
      "\n",
      "[71477 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "inputs = DATA.drop([\"Result\",\"Abnormal\"],axis='columns')\n",
    "Target = DATA[[\"Abnormal\"]]\n",
    "\n",
    "NumAbnormal=0\n",
    "for i in DATA[\"Abnormal\"]:\n",
    "    if (i==1):\n",
    "        NumAbnormal+=1\n",
    "print(NumAbnormal) #23% abnormal\n",
    "\n",
    "rows_to_remove=DATA.index[DATA['Abnormal'] == 0].tolist()\n",
    "inputs2 = inputs.drop(rows_to_remove[NumAbnormal:],axis='rows')\n",
    "Target2 = Target.drop(rows_to_remove[NumAbnormal:],axis='rows')\n",
    "\n",
    "inputs3 = inputs.drop(rows_to_remove,axis='rows')\n",
    "Target3 = Target.drop(rows_to_remove,axis='rows')\n",
    "\n",
    "inputs4 = DATA.drop([\"Result\",\"Abnormal\",\"R Height\",\"Q Height\", \"P Height\"],axis='columns')\n",
    "Target4 = DATA[[\"Abnormal\"]]\n",
    "\n",
    "print(len(inputs2))\n",
    "\n",
    "print(Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85561c1",
   "metadata": {},
   "source": [
    "Training and testing data is then created for each of the datasets so that the four models can be trained accordingly. the set is randomized in the train_test_split function and this is important due to the distribution in the actual dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "904dce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(inputs, Target,test_size=0.3)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(inputs2, Target2,test_size=0.3)\n",
    "x_train3, x_test3, y_train3, y_test3 = train_test_split(inputs3, Target3,test_size=0.3)\n",
    "x_train4, x_test4, y_train4, y_test4 = train_test_split(inputs4, Target4,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c643074",
   "metadata": {},
   "source": [
    "Here we train and test the first model. this model is trained on the entire dataset then tested on all of them,\n",
    "doing this shows a greater picture for the actual accuracy overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7bc58c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9326151837343779 Regular\n",
      "0.9752953813104189 50/50\n",
      "0.9582112868580356 Abnormal\n",
      "0.9087029308321065 AUC1\n",
      "0.9754375107515829 AUC2\n"
     ]
    }
   ],
   "source": [
    "model1.fit(X=x_train1,y=y_train1)\n",
    "print(model1.score(x_test1,y_test1),\"Regular\")\n",
    "print(model1.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model1.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model1.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model1.predict(x_test2)),\"AUC2\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a41c0d7",
   "metadata": {},
   "source": [
    "The second model is trained with a dataset consisting of an even distribution of normal and abnormal heart beats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5d1209ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6634489833986197 Regular\n",
      "0.9474660677668196 50/50\n",
      "0.9834016793595001 Abnormal\n",
      "0.772769409984909 AUC1\n",
      "0.9474385009461392 AUC2\n"
     ]
    }
   ],
   "source": [
    "#for inputs with even normal and abnormal\n",
    "\n",
    "model2.fit(X=x_train2,y=y_train2)\n",
    "print(model2.score(x_test1,y_test1),\"Regular\")\n",
    "print(model2.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model2.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model2.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model2.predict(x_test2)),\"AUC2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e2aa0",
   "metadata": {},
   "source": [
    "The third model is trained on a dataset consisting of only abnormal data, this was done mostly as a test to see what would happen by experimenting with the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a5c3234e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24244543928371573 Regular\n",
      "0.5048335123523093 50/50\n",
      "1.0 Abnormal\n",
      "0.5 AUC1\n",
      "0.5 AUC2\n"
     ]
    }
   ],
   "source": [
    "model3.fit(X=x_train3,y=y_train3)\n",
    "print(model3.score(x_test1,y_test1),\"Regular\")\n",
    "print(model3.score(x_test2,y_test2), \"50/50\",)\n",
    "print(model3.score(x_test3,y_test3),\"Abnormal\")\n",
    "print(roc_auc_score(y_test1, model3.predict(x_test1)),\"AUC1\")\n",
    "print(roc_auc_score(y_test2, model3.predict(x_test2)),\"AUC2\")\n",
    "#print(roc_auc_score(y_test3, model1.predict(x_test3)),\"AUC3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bdd6c6",
   "metadata": {},
   "source": [
    "The final model is trained on the entire dataset with the omission of peak height features. this is due to having less confidence in these values. accuracy regarding these features would likely improve with normalizing on the waves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "992284a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8792669278119754 Regular\n",
      "0.8344103330494916 AUC1\n"
     ]
    }
   ],
   "source": [
    "model4.fit(X=x_train4,y=y_train4)\n",
    "print(model4.score(x_test4,y_test4),\"Regular\")\n",
    "print(roc_auc_score(y_test4, model4.predict(x_test4)),\"AUC1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ef6ac",
   "metadata": {},
   "source": [
    "It can be seen that model1, taking all of the provided data has the highest accuracy when tested against all of the subsets of data. This was the expected conclusion but it is still useful to show proof of this fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1c6e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSNAMES=[\"0\",\"1\"]\n",
    "fig = plt.figure(figsize=(100,100))\n",
    "#_ = tree.plot_tree(model,feature_names=inputs.columns,class_names=CLASSNAMES)\n",
    "_ = tree.plot_tree(model,feature_names=inputs.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee2d66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
