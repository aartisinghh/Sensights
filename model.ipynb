{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "288d7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import neurokit2 as nk\n",
    "import csv\n",
    "import numpy as np\n",
    "#this is probably a bad idea to suppress all warnings but it gets rid of the neurokit warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b2d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_txt(txt_path):\n",
    "        '''\n",
    "        Args:\n",
    "            txt_path (str): path to the annotations.txt file\n",
    "        Returns:\n",
    "            Pandas df: dataframe with the first three columns of the text file (\"Time\", \"Sample#\", \"Result\")\n",
    "        '''\n",
    "        data = []\n",
    "        with open(txt_path,'r') as data_file:\n",
    "            for line in data_file:\n",
    "                data.append(line.split()[:3])\n",
    "        df = pd.DataFrame(data[1:], columns = ['Time', 'Sample#', 'Result'])\n",
    "        #convert the sample to integers\n",
    "        for i in range(len(df[\"Sample#\"])):\n",
    "            df[\"Sample#\"][i] = int(df[\"Sample#\"][i])\n",
    "        return df\n",
    "    \n",
    "def NeurokitExtraction(FileNumber, signal = \"\\'MLII\\'\"):\n",
    "    \"\"\"\n",
    "    Added signal preference:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        \n",
    "        print(\"extracting File\",FileNumber)\n",
    "\n",
    "        #load data and annotations into dataframes\n",
    "        File = pd.read_csv(\"mitbih_database/\"+str(FileNumber)+\".csv\")\n",
    "        annotations = df_from_txt(\"mitbih_database/\"+str(FileNumber)+\"annotations.txt\")\n",
    "\n",
    "        #select the signal from the file\n",
    "        if signal in File.columns:\n",
    "            #preferred signal\n",
    "            ecg_signal = File[signal]\n",
    "\n",
    "        elif \"\\'V5\\'\" in File.columns:\n",
    "            #otherwise default to V5 signal\n",
    "            ecg_signal = File[\"\\'V5\\'\"]\n",
    "\n",
    "        else:\n",
    "            #otherwise default to whatever signal is there\n",
    "            ecg_signal = File[File.columns[1]].to_numpy()\n",
    "\n",
    "\n",
    "        rpeak = []\n",
    "        for i in range(len(annotations['Sample#'])):\n",
    "            x = int(annotations['Sample#'][i])\n",
    "            rpeak.append(x)\n",
    "        rpeak = np.array(rpeak)\n",
    "\n",
    "        ecg_signal = File[File.columns[1]].to_numpy()\n",
    "        cleaned_ecg = nk.ecg_clean(ecg_signal, sampling_rate=360, method='neurokit')#used\n",
    "\n",
    "        events = rpeak[1:]\n",
    "        epochs = nk.epochs_create(ecg_signal, events, sampling_rate=360, epochs_start=-0.4, epochs_end=0.4)\n",
    "\n",
    "\n",
    "        features = nk.ecg_analyze(epochs, sampling_rate=360, method='auto', subepoch_rate=[None, None])\n",
    "        features.columns = ['beat_number', 'ECG_R_Peaks']\n",
    "\n",
    "\n",
    "\n",
    "        dataframe, QRS = nk.ecg_delineate(cleaned_ecg, rpeaks=rpeak[1:], sampling_rate=360, method='dwt', show=False, show_type='all', check=False)\n",
    "\n",
    "        features['ECG_Q_Peaks'] = QRS['ECG_Q_Peaks']\n",
    "        features['ECG_S_Peaks'] = QRS['ECG_S_Peaks']\n",
    "        features['ECG_P_Peaks'] = QRS['ECG_P_Peaks']\n",
    "        features['ECG_P_Onsets'] = QRS['ECG_P_Onsets']\n",
    "        features['ECG_P_Offsets'] = QRS['ECG_P_Offsets']\n",
    "        features['ECG_T_Peaks'] = QRS['ECG_T_Peaks']\n",
    "        features['ECG_T_Onsets'] = QRS['ECG_T_Onsets']\n",
    "        features['ECG_T_Offsets'] = QRS['ECG_T_Offsets']\n",
    "        features['ECG_R_Onsets'] = QRS['ECG_R_Onsets']\n",
    "        features['ECG_R_Offsets'] = QRS['ECG_R_Offsets']\n",
    "\n",
    "        print(\"{} Extracted\".format(FileNumber))\n",
    "    except:\n",
    "        print(\"Not able to process file:\", FileNumber)\n",
    "        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57d5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 114, 115, 116, 118,\n",
    "         119, 121, 122, 123, 124, 200, 201, 202, 203, 205, 207, 208, 209, 210, 212, 213, 214,\n",
    "         215, 217, 219, 220, 221, 222, 223, 228, 230, 231, 232, 233, 234]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19f5916f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting File 100\n",
      "100 Extracted\n",
      "extracting File 101\n",
      "101 Extracted\n",
      "extracting File 102\n",
      "102 Extracted\n",
      "extracting File 103\n",
      "103 Extracted\n",
      "extracting File 104\n",
      "104 Extracted\n",
      "extracting File 105\n",
      "105 Extracted\n",
      "extracting File 106\n",
      "106 Extracted\n",
      "extracting File 107\n",
      "107 Extracted\n",
      "extracting File 108\n",
      "108 Extracted\n",
      "extracting File 109\n",
      "109 Extracted\n",
      "extracting File 111\n",
      "111 Extracted\n",
      "extracting File 112\n",
      "112 Extracted\n",
      "extracting File 114\n",
      "114 Extracted\n",
      "extracting File 115\n",
      "115 Extracted\n",
      "extracting File 116\n",
      "116 Extracted\n",
      "extracting File 118\n",
      "118 Extracted\n",
      "extracting File 119\n",
      "119 Extracted\n",
      "extracting File 121\n",
      "121 Extracted\n",
      "extracting File 122\n",
      "122 Extracted\n",
      "extracting File 123\n",
      "123 Extracted\n",
      "extracting File 124\n",
      "124 Extracted\n",
      "extracting File 200\n",
      "200 Extracted\n",
      "extracting File 201\n",
      "201 Extracted\n",
      "extracting File 202\n",
      "202 Extracted\n",
      "extracting File 203\n",
      "203 Extracted\n",
      "extracting File 205\n",
      "205 Extracted\n",
      "extracting File 207\n",
      "207 Extracted\n",
      "extracting File 208\n",
      "208 Extracted\n",
      "extracting File 209\n",
      "209 Extracted\n",
      "extracting File 210\n",
      "210 Extracted\n",
      "extracting File 212\n",
      "212 Extracted\n",
      "extracting File 213\n",
      "213 Extracted\n",
      "extracting File 214\n",
      "214 Extracted\n",
      "extracting File 215\n",
      "215 Extracted\n",
      "extracting File 217\n",
      "217 Extracted\n",
      "extracting File 219\n",
      "219 Extracted\n",
      "extracting File 220\n",
      "220 Extracted\n",
      "extracting File 221\n",
      "221 Extracted\n",
      "extracting File 222\n",
      "222 Extracted\n",
      "extracting File 223\n",
      "223 Extracted\n",
      "extracting File 228\n",
      "228 Extracted\n",
      "extracting File 230\n",
      "230 Extracted\n",
      "extracting File 231\n",
      "231 Extracted\n",
      "extracting File 232\n",
      "232 Extracted\n",
      "extracting File 233\n",
      "233 Extracted\n",
      "extracting File 234\n",
      "234 Extracted\n"
     ]
    }
   ],
   "source": [
    "testFeatures=pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    DATA = pd.DataFrame(np.nan, index=range(1), columns = testFeatures.columns)\n",
    "    testFeatures = pd.concat([testFeatures,DATA])\n",
    "    testFeatures = pd.concat([testFeatures,NeurokitExtraction(file)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59976f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeatures.to_csv(\"testFeatures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "737f44f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training = pd.DataFrame(columns = ['File','Result', 'Sample#'] )\n",
    "\n",
    "for i in files:\n",
    "    temp= pd.DataFrame(columns = Training.columns)\n",
    "\n",
    "    FileName =\"mitbih_database/\" + str(i)+'annotations.txt'\n",
    "    annotations = df_from_txt(FileName)\n",
    "    temp['Sample#']=annotations['Sample#']\n",
    "    temp['Result']=annotations['Result']\n",
    "    temp['File']= i\n",
    "    \n",
    "    Training = pd.concat([Training, temp], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a43443ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>File</th>\n",
       "      <th>Result</th>\n",
       "      <th>Sample#</th>\n",
       "      <th>beat_number</th>\n",
       "      <th>ECG_R_Peaks</th>\n",
       "      <th>ECG_Q_Peaks</th>\n",
       "      <th>ECG_S_Peaks</th>\n",
       "      <th>ECG_P_Peaks</th>\n",
       "      <th>ECG_P_Onsets</th>\n",
       "      <th>ECG_P_Offsets</th>\n",
       "      <th>ECG_T_Peaks</th>\n",
       "      <th>ECG_T_Onsets</th>\n",
       "      <th>ECG_T_Offsets</th>\n",
       "      <th>ECG_R_Onsets</th>\n",
       "      <th>ECG_R_Offsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>+</td>\n",
       "      <td>18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>N</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>77.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>147.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>N</td>\n",
       "      <td>370</td>\n",
       "      <td>2</td>\n",
       "      <td>370.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>338.0</td>\n",
       "      <td>432.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>468.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>N</td>\n",
       "      <td>662</td>\n",
       "      <td>3</td>\n",
       "      <td>662.0</td>\n",
       "      <td>652.0</td>\n",
       "      <td>757.0</td>\n",
       "      <td>603.0</td>\n",
       "      <td>586.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>783.0</td>\n",
       "      <td>761.0</td>\n",
       "      <td>791.0</td>\n",
       "      <td>644.0</td>\n",
       "      <td>676.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>100</td>\n",
       "      <td>N</td>\n",
       "      <td>946</td>\n",
       "      <td>4</td>\n",
       "      <td>946.0</td>\n",
       "      <td>936.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>863.0</td>\n",
       "      <td>890.0</td>\n",
       "      <td>1064.0</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>1070.0</td>\n",
       "      <td>925.0</td>\n",
       "      <td>961.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index File Result Sample# beat_number  ECG_R_Peaks  ECG_Q_Peaks  \\\n",
       "0      0  100      +      18         NaN          NaN          NaN   \n",
       "1      1  100      N      77           1         77.0         66.0   \n",
       "2      2  100      N     370           2        370.0        359.0   \n",
       "3      3  100      N     662           3        662.0        652.0   \n",
       "4      4  100      N     946           4        946.0        936.0   \n",
       "\n",
       "   ECG_S_Peaks  ECG_P_Peaks  ECG_P_Onsets  ECG_P_Offsets  ECG_T_Peaks  \\\n",
       "0          NaN          NaN           NaN            NaN          NaN   \n",
       "1         88.0          NaN           NaN            NaN        144.0   \n",
       "2        472.0        309.0         294.0          338.0        432.0   \n",
       "3        757.0        603.0         586.0          610.0        783.0   \n",
       "4        958.0        884.0         863.0          890.0       1064.0   \n",
       "\n",
       "   ECG_T_Onsets  ECG_T_Offsets  ECG_R_Onsets  ECG_R_Offsets  \n",
       "0           NaN            NaN           NaN            NaN  \n",
       "1           NaN          147.0           NaN           91.0  \n",
       "2         418.0          468.0         350.0          387.0  \n",
       "3         761.0          791.0         644.0          676.0  \n",
       "4        1045.0         1070.0         925.0          961.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NewTraining=pd.concat([NewTraining,NewFeatures],axis=1)\n",
    "Training = Training.reset_index(drop=True)\n",
    "testFeatures=testFeatures.reset_index(drop=True)\n",
    "Training=pd.concat([Training,testFeatures],axis=1)\n",
    "Training = Training.reset_index()\n",
    "Training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32bd9d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109312\n",
      "91465\n"
     ]
    }
   ],
   "source": [
    "Training = Training.drop([\"ECG_T_Peaks\",\"ECG_T_Onsets\",\"ECG_T_Offsets\",\"ECG_R_Offsets\"],axis='columns')\n",
    "print(len(Training))\n",
    "Training.dropna(inplace=True)\n",
    "print(len(Training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe47f022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading File 100\n",
      "Reading File 101\n",
      "Reading File 102\n",
      "Reading File 103\n",
      "Reading File 104\n",
      "Reading File 105\n",
      "Reading File 106\n",
      "Reading File 107\n",
      "Reading File 108\n",
      "Reading File 109\n",
      "Reading File 111\n",
      "Reading File 112\n",
      "Reading File 114\n",
      "Reading File 115\n",
      "Reading File 116\n",
      "Reading File 118\n",
      "Reading File 119\n",
      "Reading File 121\n",
      "Reading File 122\n",
      "Reading File 123\n",
      "Reading File 124\n",
      "Reading File 200\n",
      "Reading File 201\n",
      "Reading File 202\n",
      "Reading File 203\n",
      "Reading File 205\n",
      "Reading File 207\n",
      "Reading File 208\n",
      "Reading File 209\n",
      "Reading File 210\n",
      "Reading File 212\n",
      "Reading File 213\n",
      "Reading File 214\n",
      "Reading File 215\n",
      "Reading File 217\n",
      "Reading File 219\n",
      "Reading File 220\n",
      "Reading File 221\n",
      "Reading File 222\n",
      "Reading File 223\n",
      "Reading File 228\n",
      "Reading File 230\n",
      "Reading File 231\n",
      "Reading File 232\n",
      "Reading File 233\n",
      "Length of DataFrame 82529\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Abnormal</th>\n",
       "      <th>Result</th>\n",
       "      <th>R-R Interval</th>\n",
       "      <th>R Height</th>\n",
       "      <th>R_Onset-Rpeak</th>\n",
       "      <th>Q-Q Interval</th>\n",
       "      <th>Q Height</th>\n",
       "      <th>P_Onset-P_Offset</th>\n",
       "      <th>P Height</th>\n",
       "      <th>P-P Interval</th>\n",
       "      <th>P_Onset-Ppeak</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1212.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>922.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>983.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>895.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>285.0</td>\n",
       "      <td>1188.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>285.0</td>\n",
       "      <td>921.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>973.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>284.0</td>\n",
       "      <td>1201.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>915.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>974.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>294.0</td>\n",
       "      <td>1213.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>984.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>A</td>\n",
       "      <td>235.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>914.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>978.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>358.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>920.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>972.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>304.0</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>918.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>975.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>292.0</td>\n",
       "      <td>1209.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>293.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    File  Abnormal Result  R-R Interval  R Height  R_Onset-Rpeak  \\\n",
       "0  100.0       0.0      N           0.0    1212.0           20.0   \n",
       "1  100.0       0.0      N         292.0    1201.0           18.0   \n",
       "2  100.0       0.0      N         284.0    1186.0           21.0   \n",
       "3  100.0       0.0      N         285.0    1188.0           18.0   \n",
       "4  100.0       0.0      N         284.0    1201.0           20.0   \n",
       "5  100.0       0.0      N         294.0    1213.0           19.0   \n",
       "6  100.0       1.0      A         235.0    1193.0           19.0   \n",
       "7  100.0       0.0      N         358.0    1193.0           20.0   \n",
       "8  100.0       0.0      N         304.0    1202.0           20.0   \n",
       "9  100.0       0.0      N         292.0    1209.0           19.0   \n",
       "\n",
       "   Q-Q Interval  Q Height  P_Onset-P_Offset  P Height  P-P Interval  \\\n",
       "0           0.0     922.0              44.0     983.0           0.0   \n",
       "1         293.0     920.0              24.0     981.0         294.0   \n",
       "2         284.0     895.0              27.0     975.0         281.0   \n",
       "3         285.0     921.0              61.0     973.0         284.0   \n",
       "4         284.0     915.0              18.0     974.0         294.0   \n",
       "5         294.0     920.0              27.0     984.0         284.0   \n",
       "6         235.0     914.0              42.0     978.0         238.0   \n",
       "7         357.0     920.0              30.0     972.0         354.0   \n",
       "8         304.0     918.0              25.0     975.0         304.0   \n",
       "9         293.0     919.0              23.0     981.0         295.0   \n",
       "\n",
       "   P_Onset-Ppeak  \n",
       "0           15.0  \n",
       "1           17.0  \n",
       "2           21.0  \n",
       "3           27.0  \n",
       "4            4.0  \n",
       "5           20.0  \n",
       "6           23.0  \n",
       "7           19.0  \n",
       "8           18.0  \n",
       "9           17.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Interval = pd.DataFrame(columns=[\"File\",\"Abnormal\",\"Result\",\"R-R Interval\",\"R Height\",\"R_Onset-Rpeak\",\"Q-Q Interval\",\"Q Height\",\"P_Onset-P_Offset\",\"P Height\",\"P-P Interval\",\"P_Onset-Ppeak\"])\n",
    "RRLIST=np.array([])\n",
    "QQLIST=np.array([])\n",
    "R_Height=np.array([])\n",
    "Q_Height=np.array([]) \n",
    "P_Height=np.array([])\n",
    "Ron_RP=np.array([])\n",
    "Pon_PP=np.array([])\n",
    "Pon_Poff=np.array([])\n",
    "PPLIST=np.array([])\n",
    "Abnormal=np.array([])\n",
    "Result=np.array([])\n",
    "FILES=np.array([])\n",
    "y=2\n",
    "FileNumber=Training[\"File\"][y]\n",
    "RAWFile= pd.read_csv(\"mitbih_database/\"+str(FileNumber)+\".csv\")\n",
    "\n",
    "\n",
    "for i in Training[\"index\"][:-1]:\n",
    "    if (i-y>1):\n",
    "        y=i\n",
    "        continue\n",
    "    if Training[\"Result\"][i]=='N':\n",
    "        Abnormal=np.append(Abnormal,0)\n",
    "    else:\n",
    "        Abnormal=np.append(Abnormal,1)\n",
    "    if (Training[\"File\"][i] != FileNumber):\n",
    "        print(\"Reading File\",FileNumber)\n",
    "        FileNumber=Training[\"File\"][i]\n",
    "        RAWFile= pd.read_csv(\"mitbih_database/\"+str(FileNumber)+\".csv\")\n",
    "        \n",
    "    RRLIST=np.append(RRLIST,Training[\"ECG_R_Peaks\"][i]-Training[\"ECG_R_Peaks\"][y])\n",
    "    QQLIST=np.append(QQLIST,Training[\"ECG_Q_Peaks\"][i]-Training[\"ECG_Q_Peaks\"][y])\n",
    "    PPLIST=np.append(PPLIST,Training[\"ECG_P_Peaks\"][i]-Training[\"ECG_P_Peaks\"][y])\n",
    "    R_Height=np.append(R_Height,RAWFile[RAWFile.columns[1]][Training[\"ECG_R_Peaks\"][i]])#\n",
    "    Q_Height=np.append(Q_Height,RAWFile[RAWFile.columns[1]][Training[\"ECG_Q_Peaks\"][i]])#\n",
    "    P_Height=np.append(P_Height,RAWFile[RAWFile.columns[1]][Training[\"ECG_P_Peaks\"][i]])#\n",
    "    Ron_RP=np.append(Ron_RP,Training[\"ECG_R_Peaks\"][i]-Training[\"ECG_R_Onsets\"][i])\n",
    "    Pon_PP=np.append(Pon_PP,Training[\"ECG_P_Peaks\"][i]-Training[\"ECG_P_Onsets\"][i])\n",
    "    Pon_Poff=np.append(Pon_Poff,Training[\"ECG_P_Offsets\"][i]-Training[\"ECG_P_Onsets\"][i])\n",
    "    Result=np.append(Result,Training[\"Result\"][i])\n",
    "    FILES=np.append(FILES,FileNumber)\n",
    "    \n",
    "    y=i\n",
    "\n",
    "Interval[\"R-R Interval\"]=RRLIST\n",
    "Interval[\"Q-Q Interval\"]=QQLIST\n",
    "Interval[\"P-P Interval\"]=PPLIST\n",
    "Interval[\"R_Onset-Rpeak\"]=Ron_RP\n",
    "Interval[\"P_Onset-Ppeak\"]=Pon_PP\n",
    "Interval[\"P_Onset-P_Offset\"]=Pon_Poff\n",
    "Interval[\"R Height\"]=R_Height#\n",
    "Interval[\"Q Height\"]=Q_Height#\n",
    "Interval[\"P Height\"]=P_Height#\n",
    "Interval[\"File\"]=FILES\n",
    "Interval[\"Abnormal\"]=Abnormal\n",
    "Interval[\"Result\"]=Result\n",
    "\n",
    "print(\"Length of DataFrame\",len(Interval))\n",
    "Interval.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "118c3a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(Interval.drop(['Result', 'Abnormal', 'File'], axis = 'columns'), Interval['Abnormal'], test_size = 0.3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a24c9f71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bd41591c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9436164627004322"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "073d8aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9050850195888364"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying a classic decision tree:\n",
    "model1 = tree.DecisionTreeClassifier()\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "model1.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd7718",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
